{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement\n",
    "- Get Total Quantity and Total Sales by week, by store, by Department, Category etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading big data\n",
    "- The data is not available in git repo as it is too big\n",
    "- Download it before the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_products = pd.read_parquet('./data/PRODUCTS/products.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores = pd.read_parquet('./data/STORES/stores.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_parquet('./data/TRANSACTIONS/part-000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions[\"TransactionDate\"] = pd.to_datetime(df_transactions['TransactionDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below code on a 16gb, 4 core cpu mac takes more than 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_merge_tran_product = pd.merge(df_transactions, df_products, how='left', left_on=['UPC'], right_on = ['UPC'])\n",
    "del df_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.1f' % x)\n",
    "df_merge_tran_product.groupby(['CATEGORY','week_number_of_year'])['Qty','SoldRate'].sum().rename(columns={'Qty':'Total Qty','SoldRate' : 'Total Sales'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Python packaging for Spark is not intended to replace all of the other use cases. \n",
    "# This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. \n",
    "# You can download the full version of Spark from the Apache Spark downloads page.\n",
    "!pip3 install pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below variables are to be set in the shell profile\n",
    "# export SPARK_HOME=/Users/pmacharl/spark-2.4.4-bin-hadoop2.7\n",
    "# export PATH=$PATH:$SPARK_HOME/bin\n",
    "# export PYSPARK_SUBMIT_ARGS=\"pyspark-shell\"\n",
    "# export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3\n",
    "# export PYSPARK_PYTHON=/usr/local/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x10920b9a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkConf\n",
    "config = SparkConf()\n",
    "config.set(\"spark.driver.memory\", \"2g\")\n",
    "config.set(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "#Because you are likely running in local mode, it is a good practice to set the number of shuffle partitions\n",
    "# to something that is going to fit local mode. By default, the value is 200, but there aren't many executors\n",
    "# on this machine, its worth reducing this to 5\n",
    "config.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "# Cluster mode\n",
    "# https://spark.apache.org/docs/latest/submitting-applications.html\n",
    "# config.setMaster(\"spark://192.168.0.7:7077\") # If spark is started in local cluster mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115ef1490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=config).master(\"local\").appName(\"MyApp\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = spark.read.load(\"./data/PRODUCTS\")\n",
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = spark.read.load(\"./data/TRANSACTIONS\")\n",
    "df_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.createOrReplaceTempView(\"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_product_transactions = df_transactions.join(df_products, df_transactions.UPC == df_products.UPC, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|          DEPARTMENT|count(Qty)|       sum(SoldRate)|\n",
      "+--------------------+----------+--------------------+\n",
      "|              Liquor|  42612676| 4.681069539636936E8|\n",
      "|            Pharmacy|    496880|   4040946.220001863|\n",
      "|Deli and Foodservice|  22348130|1.0888377814712748E8|\n",
      "|             Grocery| 565333021| 1.514952610701241E9|\n",
      "|             Produce| 237301076| 5.579940860804292E8|\n",
      "|                 HBC|  23985791|1.3116028896553218E8|\n",
      "|             Seafood|   2209534|1.1060811070005694E7|\n",
      "|       Packaged Deli|  37220184|1.2202639257678652E8|\n",
      "|                  GM|  70948513|3.8092355814723516E8|\n",
      "|                Meat|  14072935| 7.245627896076763E7|\n",
      "|               Dairy| 119286406|3.3135169825598425E8|\n",
      "|              Frozen|  85128987|3.0558685919974375E8|\n",
      "|          major misc| 142043830| 1.049723286940231E9|\n",
      "|              Bakery|   7236959|2.7038583390471153E7|\n",
      "|              Floral|   1194587|   3493103.259998988|\n",
      "+--------------------+----------+--------------------+\n",
      "\n",
      "CPU times: user 16.7 ms, sys: 10.8 ms, total: 27.5 ms\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_join_product_transactions.groupby(\"DEPARTMENT\").agg({\"SoldRate\": \"sum\", \"Qty\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark SQL\n",
    "- [Spark SQL JOINS](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_tran_products = spark.sql(\"select p.DEPARTMENT as Department, count(t.Qty) as TotalQty, sum(t.SoldRate) as TotalSales from transactions t join products p on t.UPC=p.UPC GROUP BY p.DEPARTMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql.types import *\n",
    "df_join_tran_products.withColumn('TotalSales', df_join_tran_products.TotalSales.cast(DecimalType(18, 2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating more data \n",
    "- Use Dbeaver (version < 6.2 , as they made \"Generate mock data\" feature enterprise from 6.2)\n",
    "- Generate mock data is intuitive to use (play with it for some time and it will be easier)\n",
    "- Understanding the sequence of loading tables ensures PK-FK validations are automatically taken care of for e.g. generating store records with address_ids that don't exist in address table doesn't make sense (so select FK for address_id field)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
