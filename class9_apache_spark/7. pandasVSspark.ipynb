{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirement\n",
    "- Get Total Quantity and Total Sales by week, by store, by Department, Category etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading big data\n",
    "- The data is not available in git repo as it is too big\n",
    "- Download it before the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df_products = pd.read_parquet('./data/PRODUCTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stores = pd.read_parquet('./data/STORES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_parquet('./data/TRANSACTIONS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions[\"TransactionDate\"] = pd.to_datetime(df_transactions['TransactionDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below code on a 16gb, 4 core cpu mac takes more than 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_merge_tran_product = pd.merge(df_transactions, df_products, how='left', left_on=['UPC'], right_on = ['UPC'])\n",
    "del df_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.1f' % x)\n",
    "df_merge_tran_product.groupby(['CATEGORY','week_number_of_year'])['Qty','SoldRate'].sum().rename(columns={'Qty':'Total Qty','SoldRate' : 'Total Sales'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Python packaging for Spark is not intended to replace all of the other use cases. \n",
    "# This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. \n",
    "# You can download the full version of Spark from the Apache Spark downloads page.\n",
    "! pip install pyspark==2.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below variables are to be set in the shell profile\n",
    "# export SPARK_HOME=/Users/pmacharl/spark-2.4.4-bin-hadoop2.7\n",
    "# export PATH=$PATH:$SPARK_HOME/bin\n",
    "# export PYSPARK_SUBMIT_ARGS=\"pyspark-shell\"\n",
    "# export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3\n",
    "# export PYSPARK_PYTHON=/usr/local/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x109108950>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkConf\n",
    "config = SparkConf()\n",
    "config.set(\"spark.driver.memory\", \"2g\")\n",
    "config.set(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "#Because you are likely running in local mode, it is a good practice to set the number of shuffle partitions\n",
    "# to something that is going to fit local mode. By default, the value is 200, but there aren't many executors\n",
    "# on this machine, its worth reducing this to 5\n",
    "config.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "# Cluster mode\n",
    "# https://spark.apache.org/docs/latest/submitting-applications.html\n",
    "# config.setMaster(\"spark://192.168.0.7:7077\") # If spark is started in local cluster mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10ebc9710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=config).master(\"local\").appName(\"MyApp\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = spark.read.load(\"./data/PRODUCTS\")\n",
    "df_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.createOrReplaceTempView(\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = spark.read.load(\"./data/TRANSACTIONS\")\n",
    "df_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.createOrReplaceTempView(\"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_product_transactions = df_transactions.join(df_products, df_transactions.UPC == df_products.UPC, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_join_product_transactions.groupby(\"DEPARTMENT\").agg({\"SoldRate\": \"sum\", \"Qty\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark SQL\n",
    "- [Spark SQL JOINS](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-joins.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join_tran_products = spark.sql(\"select p.DEPARTMENT as Department, count(t.Qty) as TotalQty, sum(t.SoldRate) as TotalSales from transactions t join products p on t.UPC=p.UPC GROUP BY p.DEPARTMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql.types import *\n",
    "df_join_tran_products.withColumn('TotalSales', df_join_tran_products.TotalSales.cast(DecimalType(18, 2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating more data \n",
    "- Use Dbeaver (version < 6.2 , as they made \"Generate mock data\" feature enterprise from 6.2)\n",
    "- Generate mock data is intuitive to use (play with it for some time and it will be easier)\n",
    "- Understanding the sequence of loading tables ensures PK-FK validations are automatically taken care of for e.g. generating store records with address_ids that don't exist in address table doesn't make sense (so select FK for address_id field)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
