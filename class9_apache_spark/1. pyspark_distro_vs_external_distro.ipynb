{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method1: pyspark not installed and use findspark\n",
    "- Let findspark find it for you\n",
    "- The download folder `spark-3.0.0-bin-hadoop2.7` contains python and pyspark\n",
    "- Use findspark to initialize (Provides findspark.init() to make pyspark importable as a regular library.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark download contains pyspark in it, use findspark to locate and initialize that. Don't need to explicitly install pyspark\n",
    "!pip3 install findspark==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below variables are to be set in the shell profile\n",
    "# export SPARK_HOME=/Users/pmacharl/spark-3.0.0-bin-hadoop2.7\n",
    "# export PATH=$PATH:$SPARK_HOME/bin\n",
    "# export PYSPARK_SUBMIT_ARGS=\"pyspark-shell\"\n",
    "# export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3\n",
    "# export PYSPARK_PYTHON=/usr/local/bin/python3\n",
    "# export PYSPARK_DRIVER_PYTHON_OPTS=notebook\n",
    "# export PYARROW_IGNORE_TIMEZONE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>some-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11a9f3be0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.appName(\"some-example\").getOrCreate()\n",
    "sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method2: Install pyspark, don't use findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.0.0 in /Users/pmacharl/git-projects/personal/github.com/data_analysis_pandas_spark_koalas/venv/lib/python3.8/site-packages (3.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9 in /Users/pmacharl/git-projects/personal/github.com/data_analysis_pandas_spark_koalas/venv/lib/python3.8/site-packages (from pyspark==3.0.0) (0.10.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/Users/pmacharl/git-projects/personal/github.com/data_analysis_pandas_spark_koalas/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>some-app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115854820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "config = SparkConf()\n",
    "config.set(\"spark.driver.memory\", \"2g\")\n",
    "config.set(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=config).master(\"local\").appName(\"some-app\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method3: Install pyspark, explicitly start standalone spark server (cluster mode)\n",
    "- and connect to it\n",
    "- Cluster mode enables you to scale spark server independently\n",
    "- There is a default cluster manager that comes out of box. Other cluster managers like k8s, mesos can be integrated and that is a separate learning for large hyperscale environments\n",
    "- From `$SPARK_HOME/bin` execute `./sbin/start-all.sh`. More [options](https://spark.apache.org/docs/latest/spark-standalone.html) for passing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below variables are to be set in the shell profile\n",
    "# export SPARK_HOME=/Users/pmacharl/spark-3.0.0-bin-hadoop2.7\n",
    "# export PATH=$PATH:$SPARK_HOME/bin\n",
    "# export PYSPARK_SUBMIT_ARGS=\"pyspark-shell\"\n",
    "# export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3\n",
    "# export PYSPARK_PYTHON=/usr/local/bin/python3\n",
    "# export PYSPARK_DRIVER_PYTHON_OPTS=notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of below, you could use the pyspark package that comes along with $SPARK_HOME folder, but that \n",
    "# would mean you have to set PYTHONPATH variable so that python can look at additional places for packages\n",
    "# and not just site-packages folder. \n",
    "# For e.g. you can set cd spark-3.3.0-bin-hadoop3\n",
    "# export SPARK_HOME=`pwd`\n",
    "# export PYTHONPATH=$(ZIPS=(\"$SPARK_HOME\"/python/lib/*.zip); IFS=:; echo \"${ZIPS[*]}\"):$PYTHONPATH\n",
    "\n",
    "!pip3 install pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>some-app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115854df0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "config = SparkConf()\n",
    "config.set(\"spark.driver.memory\", \"2g\")\n",
    "config.set(\"spark.executor.memory\", \"1g\")\n",
    "config.setMaster(\"spark://192.168.0.4:7077\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=config).master(\"local\").appName(\"some-app\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
