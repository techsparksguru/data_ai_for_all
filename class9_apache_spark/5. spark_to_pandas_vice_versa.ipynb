{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkConf\n",
    "config = SparkConf()\n",
    "config.set(\"spark.driver.memory\", \"2g\")\n",
    "config.set(\"spark.executor.memory\", \"1g\")\n",
    "#Because you are likely running in local mode, it is a good practice to set the number of shuffle partitions\n",
    "# to something that is going to fit local mode. By default, the value is 200, but there aren't many executors\n",
    "# on this machine, its worth reducing this to 5\n",
    "config.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark = SparkSession.builder.config(conf=config).master(\"local\").appName(\"Analyzing Real Estate Sales\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option(\"header\", \"true\").load('../monthly_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark dataframe to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "pandas_df = df.select(\"*\").toPandas()\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas to spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n",
    "# Not every conversion will go fine, especially if data types don't match up. See load_various_formats file\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting spark 3.3\n",
    " - Koalas code has been merged\n",
    " - Pandas-on-Spark is a new datastructure that is a distributed version of Pandas dataframe (so can use pandas syntax for most parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Pandas-on-Spark (If your Spark Context / Spark Session already exists, it will be picked up by default)\n",
    "import pyspark.pandas as ps\n",
    "df = spark.read.format('csv').option(\"header\", \"true\").load('../monthly_data.csv')\n",
    "\n",
    "# Create a DataFrame with Pandas-on-Spark\n",
    "ps_df = ps.DataFrame(df)\n",
    "# Convert a Pandas-on-Spark Dataframe into a Pandas Dataframe\n",
    "pd_df = ps_df.to_pandas()\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a Pandas Dataframe into a Pandas-on-Spark Dataframe\n",
    "ps_df = ps.from_pandas(pd_df)\n",
    "ps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.catalog.clearCache()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
