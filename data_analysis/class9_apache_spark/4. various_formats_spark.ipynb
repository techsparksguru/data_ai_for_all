{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/pmacharl/Library/Caches/pip/wheels/60/74/91/22826adce98cd838d5258b7a0b245d5369a5164d42852d9a36/pyspark-3.0.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9\n",
      "  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The Python packaging for Spark is not intended to replace all of the other use cases. \n",
    "# This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. \n",
    "# You can download the full version of Spark from the Apache Spark downloads page.\n",
    "!pip3 install pyspark==3.0.0\n",
    "!pip3 install psycopg2-binary==2.8.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below variables are to be set in the shell profile\n",
    "# export SPARK_HOME=/Users/pmacharl/spark-2.4.4-bin-hadoop2.7\n",
    "# export PATH=$PATH:$SPARK_HOME/bin\n",
    "# export PYSPARK_SUBMIT_ARGS=\"pyspark-shell\"\n",
    "# export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3\n",
    "# export PYSPARK_PYTHON=/usr/local/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x10e4b9370>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkConf\n",
    "config = SparkConf()\n",
    "config.set(\"spark.driver.memory\", \"2g\")\n",
    "config.set(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "#Because you are likely running in local mode, it is a good practice to set the number of shuffle partitions\n",
    "# to something that is going to fit local mode. By default, the value is 200, but there aren't many executors\n",
    "# on this machine, its worth reducing this to 5\n",
    "config.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "# Cluster mode\n",
    "# https://spark.apache.org/docs/latest/submitting-applications.html\n",
    "config.setMaster(\"spark://192.168.0.4:7077\") # If spark is started in local cluster mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1161bde20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=config).master(\"local\").appName(\"MyApp\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the simplest form, the default data source (parquet unless otherwise configured by spark.sql.sources.default) will be used for all operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+\n",
      "|sex|weight|height|repwt|repht|\n",
      "+---+------+------+-----+-----+\n",
      "|  M|    77|   182| 77.0|180.0|\n",
      "|  F|    58|   161| 51.0|159.0|\n",
      "|  F|    53|   161| 54.0|158.0|\n",
      "|  M|    68|   177| 70.0|175.0|\n",
      "|  F|    59|   157| 59.0|155.0|\n",
      "|  M|    76|   170| 76.0|165.0|\n",
      "|  M|    76|   167| 77.0|165.0|\n",
      "|  M|    69|   186| 73.0|180.0|\n",
      "|  M|    71|   178| 71.0|175.0|\n",
      "|  M|    65|   171| 64.0|170.0|\n",
      "|  M|    70|   175| 75.0|174.0|\n",
      "|  F|   166|    57| 56.0|163.0|\n",
      "|  F|    51|   161| 52.0|158.0|\n",
      "|  F|    64|   168| 64.0|165.0|\n",
      "|  F|    52|   163| 57.0|160.0|\n",
      "|  F|    65|   166| 66.0|165.0|\n",
      "|  M|    92|   187|101.0|185.0|\n",
      "|  F|    62|   168| 62.0|165.0|\n",
      "|  M|    76|   197| 75.0|200.0|\n",
      "|  F|    61|   175| 61.0|171.0|\n",
      "+---+------+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"./data/height_weight.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preferred way is to be explicit\n",
    "- You can also manually specify the data source that will be used along with any extra options that you would like to pass to the data source. Data sources are specified by their fully qualified name (i.e., org.apache.spark.sql.parquet), but for built-in sources you can also use their short names (json, parquet, jdbc, orc, libsvm, csv, text). DataFrames loaded from any data source type can be converted into other types using this syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+\n",
      "|sex|weight|height|repwt|repht|\n",
      "+---+------+------+-----+-----+\n",
      "|  M|    77|   182| 77.0|180.0|\n",
      "|  F|    58|   161| 51.0|159.0|\n",
      "|  F|    53|   161| 54.0|158.0|\n",
      "|  M|    68|   177| 70.0|175.0|\n",
      "|  F|    59|   157| 59.0|155.0|\n",
      "|  M|    76|   170| 76.0|165.0|\n",
      "|  M|    76|   167| 77.0|165.0|\n",
      "|  M|    69|   186| 73.0|180.0|\n",
      "|  M|    71|   178| 71.0|175.0|\n",
      "|  M|    65|   171| 64.0|170.0|\n",
      "|  M|    70|   175| 75.0|174.0|\n",
      "|  F|   166|    57| 56.0|163.0|\n",
      "|  F|    51|   161| 52.0|158.0|\n",
      "|  F|    64|   168| 64.0|165.0|\n",
      "|  F|    52|   163| 57.0|160.0|\n",
      "|  F|    65|   166| 66.0|165.0|\n",
      "|  M|    92|   187|101.0|185.0|\n",
      "|  F|    62|   168| 62.0|165.0|\n",
      "|  M|    76|   197| 75.0|200.0|\n",
      "|  F|    61|   175| 61.0|171.0|\n",
      "+---+------+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"./data/height_weight.parquet\", format=\"parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+\n",
      "|sex|weight|height|repwt|repht|\n",
      "+---+------+------+-----+-----+\n",
      "|  M|    77|   182|   77|  180|\n",
      "|  F|    58|   161|   51|  159|\n",
      "|  F|    53|   161|   54|  158|\n",
      "|  M|    68|   177|   70|  175|\n",
      "|  F|    59|   157|   59|  155|\n",
      "|  M|    76|   170|   76|  165|\n",
      "|  M|    76|   167|   77|  165|\n",
      "|  M|    69|   186|   73|  180|\n",
      "|  M|    71|   178|   71|  175|\n",
      "|  M|    65|   171|   64|  170|\n",
      "|  M|    70|   175|   75|  174|\n",
      "|  F|   166|    57|   56|  163|\n",
      "|  F|    51|   161|   52|  158|\n",
      "|  F|    64|   168|   64|  165|\n",
      "|  F|    52|   163|   57|  160|\n",
      "|  F|    65|   166|   66|  165|\n",
      "|  M|    92|   187|  101|  185|\n",
      "|  F|    62|   168|   62|  165|\n",
      "|  M|    76|   197|   75|  200|\n",
      "|  F|    61|   175|   61|  171|\n",
      "+---+------+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"../height_weight.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"./data/2015-summary.json\",\n",
    "                     format=\"json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read excel\n",
    "- Spark doesn't have in built support for excel\n",
    "- Read using other libraries and convert to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+----------------+-----+-------+--------------+--------------------+\n",
      "|       Business Name|         Email|            Category|          Category 2|          Category 3|             Address|            City|State| Postal|         Phone|             Website|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+----------------+-----+-------+--------------+--------------------+\n",
      "|Stone Cove Marina...| NOT IN SAMPLE|               Docks|             Marinas|       Dock Builders|   134 Salt Pond Rd |       Wakefield|   RI| 2879.0|(401) 783-8990|http://stonecovem...|\n",
      "|     Bluehaven Homes| NOT IN SAMPLE| General Contractors|       Home Builders|                 NaN|       5701 Time Sq |        Amarillo|   TX|79119.0|(806) 452-2545|http://www.blueha...|\n",
      "|Michael Jays Tatt...| NOT IN SAMPLE|            Jewelers|       Body Piercing|             Tattoos|1929 N Washington...|        Bismarck|   ND|58501.0|(701) 222-8282|http://michaeljay...|\n",
      "|Cardona-Hine Gallery| NOT IN SAMPLE|Art Galleries, De...|                 NaN|                 NaN|  82 County Road 75 |         Truchas|   NM|87578.0|(505) 689-2253|http://cardonahin...|\n",
      "|              Cancun| NOT IN SAMPLE| Mexican Restaurants|                 NaN|                 NaN|   2134 Allston Way |        Berkeley|   CA|94704.0|(510) 549-0964|http://www.saborm...|\n",
      "|               @cafe| NOT IN SAMPLE|        Coffee Shops|                 NaN|                 NaN|  716 Rainier Ave S |         Seattle|   WA|98144.0|(206) 328-5283|                 NaN|\n",
      "|Stone County Econ...| NOT IN SAMPLE|Economic Developm...|                 NaN|                 NaN|   115 Hatten Ave E |         Wiggins|   MS|39577.0|(601) 928-5418|http://stonecount...|\n",
      "|Anthony's Custom ...| NOT IN SAMPLE|             Tailors|Clothing Alterations|                 NaN|   1025 Quarrier St |      Charleston|   WV|25301.0|(304) 720-4858|http://bestmaster...|\n",
      "|Sever-Clark Count...| NOT IN SAMPLE|           Libraries|                 NaN|                 NaN|  207 W Chestnut St |          Kahoka|   MO|63445.0|(660) 727-3262|http://mogenclark...|\n",
      "|Bunkers Music Bar...| NOT IN SAMPLE|         Restaurants|         Bar  Grills|                Bars|761 Washington Av...|     Minneapolis|   MN|55401.0|(612) 338-8188|http://www.bunker...|\n",
      "|    Pancho O Malleys| NOT IN SAMPLE| Mexican Restaurants|                 NaN|                 NaN|140 Point Judith Rd |    Narragansett|   RI| 2882.0|(401) 782-2299|http://www.pancho...|\n",
      "|         Vapor Queen| NOT IN SAMPLE|Cigar, Cigarette ...|                 NaN|                 NaN|        171 Main St |  Alexander City|   AL|35010.0|(256) 392-4422|http://vaporqueen...|\n",
      "|Mac Nabb Photography| NOT IN SAMPLE|Wedding Photograp...|                 NaN|                 NaN|  102 Apple Hill Rd |         Hatboro|   PA|19040.0|(215) 674-8655|http://macnabbpho...|\n",
      "|Rick's New York S...| NOT IN SAMPLE|         Restaurants|               Pizza|Take Out Restaurants| 1320 Lakewood Mall |            Lodi|   CA|95242.0|(209) 400-2398|http://www.ricksp...|\n",
      "| Quik Wok Restaurant| NOT IN SAMPLE| Chinese Restaurants|   Asian Restaurants|                 NaN|      361 W Main St |            Kuna|   ID|83634.0|(208) 922-4088|http://www.quikwo...|\n",
      "|    Askar Management| NOT IN SAMPLE|         Restaurants|               Pizza|                 NaN|2525 S Telegraph Rd |Bloomfield Hills|   MI|48302.0|(248) 888-7272|http://askarbrand...|\n",
      "|Micanopy Middle S...| NOT IN SAMPLE|      Middle Schools|      Public Schools|             Schools|708 NW Okehumkee St |        Micanopy|   FL|32667.0|(352) 466-1090|http://micanopymi...|\n",
      "|Commander Restaur...|NOT IN SAMPLE |                Bars|             Taverns|American Restaurants|   30279 Airport Rd |    Pequot Lakes|   MN|56472.0|(218) 562-4198|http://www.comman...|\n",
      "|   Burke Town School| NOT IN SAMPLE|      Public Schools|             Schools|                 NaN|3293 Burke Hollow...|      West Burke|   VT| 5871.0|(802) 467-3385|http://burke.cnsu...|\n",
      "|Great Wall Billiards| NOT IN SAMPLE|                Bars|          Pool Halls|         Restaurants|7064 Spring Garde...|     Springfield|   VA|22150.0|(703) 866-7788|http://greatwallb...|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+----------------+-----+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TypeError: field Category 3: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\n",
    "# To address conversion of data types issues, we need to explicitly define schema\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "pandas_df = pd.read_excel('./data/usa_email_sample_db.xlsx')\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/types.html\n",
    "mySchema = StructType([ StructField(\"Business Name\", StringType(), True)\\\n",
    "                      ,StructField(\"Email\", StringType(), True)\\\n",
    "                      ,StructField(\"Category\", StringType(), True)\\\n",
    "                      ,StructField(\"Category 2\", StringType(), True)\\\n",
    "                      ,StructField(\"Category 3\", StringType(), True)\\\n",
    "                      ,StructField(\"Address\", StringType(), True)\\\n",
    "                      ,StructField(\"City\", StringType(), True)\\\n",
    "                      ,StructField(\"State\", StringType(), True)\\\n",
    "                      ,StructField(\"Postal\", DoubleType(), True)\\\n",
    "                      ,StructField(\"Phone\", StringType(), True)\\\n",
    "                      ,StructField(\"Website\", StringType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(pandas_df, schema=mySchema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+--------------------+--------------------+---------+--------------------+--------+-----------------+----+-----+---+\n",
      "|ScrapeDate|Manufacturer|     Retailer|               Brand|          CleanBrand|FaceValue|        OfferDetails| Expires|__index_level_0__|year|month|day|\n",
      "+----------+------------+-------------+--------------------+--------------------+---------+--------------------+--------+-----------------+----+-----+---+\n",
      "|04/20/2020|          NA|DollarGeneral|      Dollar General|      Dollar General|     5.00|SAVE $5.00 when y...|04/25/20|                1|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|      Dollar General|      Dollar General|     2.00|SAVE $2.00 when y...|01/31/21|                2|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|                Gain|                Gain|     1.00|SAVE $1.00 on ONE...|04/25/20|                3|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|  TIDE LAUNDRY GROUP|  TIDE LAUNDRY GROUP|      0.5|SAVE 50¢ on ONE T...|04/25/20|                4|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|          Coca-Cola®|           Coca Cola|     1.00|SAVE $1.00 on ANY...|06/14/20|                5|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|KIT KAT Snack Siz...|KIT KAT Snack Siz...|       NA|BUY 3 GET 1 FREE ...|04/30/20|                6|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|      Gatorade® 32oz|       Gatorade 32oz|       NA|BUY 4 GET 1 FREE ...|04/30/20|                7|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|  TIDE LAUNDRY GROUP|  TIDE LAUNDRY GROUP|     2.00|SAVE $2.00 on ONE...|04/25/20|                8|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|                Dawn|                Dawn|     0.50|SAVE $0.50 ONE Da...|04/25/20|                9|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|   Fabric Care Group|   Fabric Care Group|     1.00|SAVE $1.00 on ONE...|04/25/20|               10|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|  COTTONELLE®, Viva®|    COTTONELLE  Viva|     2.00|SAVE $2.00 on any...|04/25/20|               11|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|                Gain|                Gain|     1.00|SAVE $1.00 on ONE...|04/25/20|               12|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|             Swiffer|             Swiffer|     1.00|SAVE $1.00 ONE Sw...|04/25/20|               13|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|            Mr Clean|            Mr Clean|     3.00|SAVE $3.00 TWO Mr...|04/25/20|               14|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|           Aquafina®|            Aquafina|     0.50|$0.50 OFF on ONE ...|04/25/20|               15|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|TROPICANA PURE PR...|TROPICANA PURE PR...|     0.50|$0.50 OFF any ONE...|04/30/20|               16|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|                Gain|                Gain|     2.00|SAVE $2.00 on ONE...|04/25/20|               17|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|            Mr Clean|            Mr Clean|     1.00|SAVE $1.00 TWO Mr...|04/25/20|               18|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|            Gillette|            Gillette|     3.00|SAVE $3.00 ONE Gi...|04/25/20|               19|2020|    4| 20|\n",
      "|04/20/2020|          NA|DollarGeneral|               Crest|               Crest|     1.00|SAVE $1.00 ONE Cr...|04/25/20|               20|2020|    4| 20|\n",
      "+----------+------------+-------------+--------------------+--------------------+---------+--------------------+--------+-----------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"./data/retailer\",\n",
    "                     format=\"parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SQL on files directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+\n",
      "|sex|weight|height|repwt|repht|\n",
      "+---+------+------+-----+-----+\n",
      "|  M|    77|   182| 77.0|180.0|\n",
      "|  F|    58|   161| 51.0|159.0|\n",
      "|  F|    53|   161| 54.0|158.0|\n",
      "|  M|    68|   177| 70.0|175.0|\n",
      "|  F|    59|   157| 59.0|155.0|\n",
      "|  M|    76|   170| 76.0|165.0|\n",
      "|  M|    76|   167| 77.0|165.0|\n",
      "|  M|    69|   186| 73.0|180.0|\n",
      "|  M|    71|   178| 71.0|175.0|\n",
      "|  M|    65|   171| 64.0|170.0|\n",
      "|  M|    70|   175| 75.0|174.0|\n",
      "|  F|   166|    57| 56.0|163.0|\n",
      "|  F|    51|   161| 52.0|158.0|\n",
      "|  F|    64|   168| 64.0|165.0|\n",
      "|  F|    52|   163| 57.0|160.0|\n",
      "|  F|    65|   166| 66.0|165.0|\n",
      "|  M|    92|   187|101.0|185.0|\n",
      "|  F|    62|   168| 62.0|165.0|\n",
      "|  M|    76|   197| 75.0|200.0|\n",
      "|  F|    61|   175| 61.0|171.0|\n",
      "+---+------+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`./data/height_weight.parquet`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+\n",
      "|_c0|   _c1|   _c2|  _c3|  _c4|\n",
      "+---+------+------+-----+-----+\n",
      "|sex|weight|height|repwt|repht|\n",
      "|  M|    77|   182|   77|  180|\n",
      "|  F|    58|   161|   51|  159|\n",
      "|  F|    53|   161|   54|  158|\n",
      "|  M|    68|   177|   70|  175|\n",
      "|  F|    59|   157|   59|  155|\n",
      "|  M|    76|   170|   76|  165|\n",
      "|  M|    76|   167|   77|  165|\n",
      "|  M|    69|   186|   73|  180|\n",
      "|  M|    71|   178|   71|  175|\n",
      "|  M|    65|   171|   64|  170|\n",
      "|  M|    70|   175|   75|  174|\n",
      "|  F|   166|    57|   56|  163|\n",
      "|  F|    51|   161|   52|  158|\n",
      "|  F|    64|   168|   64|  165|\n",
      "|  F|    52|   163|   57|  160|\n",
      "|  F|    65|   166|   66|  165|\n",
      "|  M|    92|   187|  101|  185|\n",
      "|  F|    62|   168|   62|  165|\n",
      "|  M|    76|   197|   75|  200|\n",
      "+---+------+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Not the most recommended way. Files are better read with previous API's.\n",
    "# sql is used to read from tables (temp or global)\n",
    "df = spark.sql(\"SELECT * FROM csv.`../height_weight.csv`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`./data/height_weight.parquet`\")\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").save(\"height_weight.csv\")\n",
    "# df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"height_weight.csv\")\n",
    "# https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html - All SaveModes - append, overwrite etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the file\n",
    "import shutil\n",
    "shutil.rmtree('height_weight.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to Persistent Tables\n",
    "- DataFrames can also be saved as persistent tables into Hive metastore using the saveAsTable command. \n",
    "- Notice that an existing Hive deployment is not necessary to use this feature. \n",
    "- Spark will create a default local Hive metastore (using Derby) for you. \n",
    "- Unlike the createOrReplaceTempView command, saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore. \n",
    "- Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. \n",
    "- A DataFrame for a persistent table can be created by calling the table method on a SparkSession with the name of the table.\n",
    "- For file-based data source, e.g. text, parquet, json, etc. you can specify a custom table path via the path option, e.g. df.write.option(\"path\", \"/some/path\").saveAsTable(\"t\"). \n",
    "- When the table is dropped, the custom table path will not be removed and the table data is still there. \n",
    "- If no custom table path is specified, Spark will write data to a default table path under the warehouse directory. When the table is dropped, the default table path will be removed too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables are stored in parquet format. hive metastore gets created\n",
    "df = spark.read.load(\"../height_weight.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "df.write.option(\"path\", \"./apache_table\").saveAsTable(\"height_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read a large file - Spark vs. Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe year_count.csv is not one file, but many output csv partitions\n",
    "- Apache Spark power comes through distributed systems - many machines, many files, partitions, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = \n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = \n",
    "df = spark.read.load(\"../companies_sorted.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "df_grouped = df.groupBy(\"year founded\").agg({\"name\":\"count\"}) # .show() will only print few values. That is lazy loading\n",
    "df_grouped.write.format(\"csv\").option(\"header\", \"true\").save(\"year_count.csv\") # This will ensure entire processing is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas is designed for single machine\n",
    "- There is only one output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 1.66 s, total: 20.9 s\n",
      "Wall time: 21 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>COUNT_BY_YEAR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year founded</th>\n",
       "      <th>size range</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1451.0</th>\n",
       "      <th>5001 - 10000</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670.0</th>\n",
       "      <th>1001 - 5000</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1775.0</th>\n",
       "      <th>1 - 10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11 - 50</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51 - 200</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023.0</th>\n",
       "      <th>11 - 50</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025.0</th>\n",
       "      <th>1 - 10</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027.0</th>\n",
       "      <th>1 - 10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029.0</th>\n",
       "      <th>51 - 200</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103.0</th>\n",
       "      <th>1 - 10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1737 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           COUNT_BY_YEAR\n",
       "year founded size range                 \n",
       "1451.0       5001 - 10000              1\n",
       "1670.0       1001 - 5000               1\n",
       "1775.0       1 - 10                    1\n",
       "             11 - 50                   1\n",
       "             51 - 200                  1\n",
       "...                                  ...\n",
       "2023.0       11 - 50                   1\n",
       "2025.0       1 - 10                    4\n",
       "2027.0       1 - 10                    1\n",
       "2029.0       51 - 200                  1\n",
       "2103.0       1 - 10                    1\n",
       "\n",
       "[1737 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../companies_sorted.csv\")\n",
    "df.groupby([\"year founded\",\"size range\"]).agg(COUNT_BY_YEAR=(\"name\", \"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark JDBC\n",
    "- [Spark Docs](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)\n",
    "- Drop postgres jdbc jar in SPARK_HOME/jars\n",
    "- Download jar from [here](https://jdbc.postgresql.org/download.html)\n",
    "- For postgres 9.6, download [4.2](https://jdbc.postgresql.org/download/postgresql-42.2.12.jar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Database in Class1\n",
    "- Supports sqlalchemy\n",
    "- Connection string: dialect+driver://username:password@host:port/database\n",
    "- https://docs.sqlalchemy.org/en/13/core/engines.html\n",
    "- Up the sample database. `cd class1_explore/postgres_sample_pagila && docker-compose up` \n",
    "- Use any sql client of your choice - dbeaver, dbvisualizer, pgAdmin etc.\n",
    "- The ER diagram is in the class1_explore_postgres_sample_pagila folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "import os\n",
    "DB_USER = os.getenv('DB_USER','postgres')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD','postgres')\n",
    "CONNECTION_STRING = \"postgresql://{0}:{1}@localhost/pagila\".format(DB_USER,DB_PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10d263610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkConf\n",
    "config = SparkConf()\n",
    "config.set(\"spark.driver.memory\", \"2g\")\n",
    "config.set(\"spark.executor.memory\", \"1g\")\n",
    "spark = SparkSession.builder.config(conf=config).master(\"local\").appName(\"MyApp\").getOrCreate()\n",
    "spark\n",
    "\n",
    "# Cluster mode\n",
    "# https://spark.apache.org/docs/latest/submitting-applications.html\n",
    "# config.setMaster(\"spark://192.168.0.6:7077\") # If spark is started in local cluster mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|create_date|        last_update|active|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
      "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|          4|       2|   BARBARA|    JONES|BARBARA.JONES@sak...|         8|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|          5|       1| ELIZABETH|    BROWN|ELIZABETH.BROWN@s...|         9|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|          6|       2|  JENNIFER|    DAVIS|JENNIFER.DAVIS@sa...|        10|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|          7|       1|     MARIA|   MILLER|MARIA.MILLER@saki...|        11|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|          8|       2|     SUSAN|   WILSON|SUSAN.WILSON@saki...|        12|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|          9|       2|  MARGARET|    MOORE|MARGARET.MOORE@sa...|        13|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         10|       1|   DOROTHY|   TAYLOR|DOROTHY.TAYLOR@sa...|        14|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         11|       2|      LISA| ANDERSON|LISA.ANDERSON@sak...|        15|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         12|       1|     NANCY|   THOMAS|NANCY.THOMAS@saki...|        16|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         13|       2|     KAREN|  JACKSON|KAREN.JACKSON@sak...|        17|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         14|       2|     BETTY|    WHITE|BETTY.WHITE@sakil...|        18|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         15|       1|     HELEN|   HARRIS|HELEN.HARRIS@saki...|        19|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         16|       2|    SANDRA|   MARTIN|SANDRA.MARTIN@sak...|        20|      true| 2017-02-14|2017-02-15 04:57:20|     0|\n",
      "|         17|       1|     DONNA| THOMPSON|DONNA.THOMPSON@sa...|        21|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         18|       2|     CAROL|   GARCIA|CAROL.GARCIA@saki...|        22|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         19|       1|      RUTH| MARTINEZ|RUTH.MARTINEZ@sak...|        23|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "|         20|       2|    SHARON| ROBINSON|SHARON.ROBINSON@s...|        24|      true| 2017-02-14|2017-02-15 04:57:20|     1|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' jdbcDF2 = spark.read     .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\\n          properties={\"user\": \"username\", \"password\": \"password\"})\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jdbc:postgresql://localhost/test?user=fred&password=secret\n",
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/pagila\") \\\n",
    "    .option(\"dbtable\", \"public.customer\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"postgres\") \\\n",
    "    .load()\n",
    "\n",
    "jdbcDF.show()\n",
    "\n",
    "\"\"\" jdbcDF2 = spark.read \\\n",
    "    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\n",
    "          properties={\"user\": \"username\", \"password\": \"password\"})\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
