{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Python packaging for Spark is not intended to replace all of the other use cases. \n",
    "# This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. \n",
    "# You can download the full version of Spark from the Apache Spark downloads page.\n",
    "! pip install pyspark==2.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below variables are to be set in the shell profile\n",
    "# export SPARK_HOME=/Users/pmacharl/spark-2.4.4-bin-hadoop2.7\n",
    "# export PATH=$PATH:$SPARK_HOME/bin\n",
    "# export PYSPARK_SUBMIT_ARGS=\"pyspark-shell\"\n",
    "# export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python3\n",
    "# export PYSPARK_PYTHON=/usr/local/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x10ae42e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkConf\n",
    "config = SparkConf()\n",
    "config.set(\"spark.driver.memory\", \"2g\")\n",
    "config.set(\"spark.executor.memory\", \"1g\")\n",
    "\n",
    "#Because you are likely running in local mode, it is a good practice to set the number of shuffle partitions\n",
    "# to something that is going to fit local mode. By default, the value is 200, but there aren't many executors\n",
    "# on this machine, its worth reducing this to 5\n",
    "config.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "# Cluster mode\n",
    "# https://spark.apache.org/docs/latest/submitting-applications.html\n",
    "# config.setMaster(\"spark://192.168.0.7:7077\") # If spark is started in local cluster mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1158dfd90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=config).master(\"local\").appName(\"MyApp\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+\n",
      "|sex|weight|height|repwt|repht|\n",
      "+---+------+------+-----+-----+\n",
      "|  M|    77|   182|   77|  180|\n",
      "|  F|    58|   161|   51|  159|\n",
      "|  F|    53|   161|   54|  158|\n",
      "|  M|    68|   177|   70|  175|\n",
      "|  F|    59|   157|   59|  155|\n",
      "|  M|    76|   170|   76|  165|\n",
      "|  M|    76|   167|   77|  165|\n",
      "|  M|    69|   186|   73|  180|\n",
      "|  M|    71|   178|   71|  175|\n",
      "|  M|    65|   171|   64|  170|\n",
      "|  M|    70|   175|   75|  174|\n",
      "|  F|   166|    57|   56|  163|\n",
      "|  F|    51|   161|   52|  158|\n",
      "|  F|    64|   168|   64|  165|\n",
      "|  F|    52|   163|   57|  160|\n",
      "|  F|    65|   166|   66|  165|\n",
      "|  M|    92|   187|  101|  185|\n",
      "|  F|    62|   168|   62|  165|\n",
      "|  M|    76|   197|   75|  200|\n",
      "|  F|    61|   175|   61|  171|\n",
      "+---+------+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"../height_weight.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a temp table or view\n",
    "- Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates (Note: You can have many Spark sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"height_weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-----+\n",
      "|sex|weight|height|repwt|repht|\n",
      "+---+------+------+-----+-----+\n",
      "|  M|    77|   182|   77|  180|\n",
      "|  F|    58|   161|   51|  159|\n",
      "|  F|    53|   161|   54|  158|\n",
      "|  M|    68|   177|   70|  175|\n",
      "|  F|    59|   157|   59|  155|\n",
      "|  M|    76|   170|   76|  165|\n",
      "|  M|    76|   167|   77|  165|\n",
      "|  M|    69|   186|   73|  180|\n",
      "|  M|    71|   178|   71|  175|\n",
      "|  M|    65|   171|   64|  170|\n",
      "|  M|    70|   175|   75|  174|\n",
      "|  F|   166|    57|   56|  163|\n",
      "|  F|    51|   161|   52|  158|\n",
      "|  F|    64|   168|   64|  165|\n",
      "|  F|    52|   163|   57|  160|\n",
      "|  F|    65|   166|   66|  165|\n",
      "|  M|    92|   187|  101|  185|\n",
      "|  F|    62|   168|   62|  165|\n",
      "|  M|    76|   197|   75|  200|\n",
      "|  F|    61|   175|   61|  171|\n",
      "+---+------+------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from height_weight\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slightly more involved code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|sex|count|\n",
      "+---+-----+\n",
      "|  F|  112|\n",
      "|  M|   88|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"sex\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     112|\n",
      "|      88|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from height_weight group by sex\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Physical plan for both\n",
    "- Both are exactly the same\n",
    "- There is no performance difference between writing SQL queries or writing DataFrame code, they both “compile” to the same underlying plan that we specify in DataFrame code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[sex#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(sex#10, 200)\n",
      "   +- *(1) HashAggregate(keys=[sex#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [sex#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/pmacharl/git-projects/personal/github.com/data_analysis_pandas_spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sex:string>\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"sex\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[sex#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(sex#10, 200)\n",
      "   +- *(1) HashAggregate(keys=[sex#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [sex#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/pmacharl/git-projects/personal/github.com/data_analysis_pandas_spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<sex:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from height_weight group by sex\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Temp View\n",
    "- If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view. \n",
    "- Global temporary view is tied to a system preserved database global_temp, and we must use the qualified name to refer it, e.g. SELECT * FROM global_temp.view1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_estate = spark.read.load(\"../Real_Estate_Sales_2001-2017.csv\", \n",
    "                                 format=\"csv\", sep=\",\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a global temporary view\n",
    "df_real_estate.createGlobalTempView(\"real_estate_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+--------------------+----------+--------------------+-------------+----------+-----------------+------------+---------------+----------------+--------------------+\n",
      "|    ID|SerialNumber|ListYear|        DateRecorded|      Town|             Address|AssessedValue|SaleAmount|       SalesRatio|PropertyType|ResidentialType|      NonUseCode|             Remarks|\n",
      "+------+------------+--------+--------------------+----------+--------------------+-------------+----------+-----------------+------------+---------------+----------------+--------------------+\n",
      "|815906|      170177|    2017|04/05/1999 12:00:...|New London|      293 PEQUOT AVE|       132440|    252500|            0.525|        null|     Two Family|            null|                null|\n",
      "|     2|      900035|    2009|07/20/2010 12:00:...|   Andover|     1 DOGWOOD DRIVE|        55600|     99000|0.561616161616162| Vacant Land|             NA|              NA|                  NA|\n",
      "|     3|       14011|    2014|01/14/2015 12:00:...|   Andover|     1 JUROVATY LANE|       153100|    190000|      0.805789474| Residential|  Single Family|              NA|                  NA|\n",
      "|     4|       80009|    2008|01/21/2009 12:00:...|   Andover|         1 ROSE LANE|       116600|    138900|0.839452843772498| Residential|  Single Family|              NA|                  NA|\n",
      "|     5|       15006|    2015|11/30/2015 12:00:...|   Andover|         1 ROSE LANE|       102900|     50000|            2.058| Residential|  Single Family|14 - Foreclosure|PROPERTY WAS OWNE...|\n",
      "|     6|       20030|    2002|04/24/2003 12:00:...|   Andover|       10 BAUSOLA RD|        91800|    189900| 48.3412322274881| Residential|  Single Family|               0|                  NA|\n",
      "|815907|      173165|    2017|08/01/2001 12:00:...|   Shelton|        28 SUNSET DR|       126980|    238000|            0.534|        null|  Single Family|            null|                null|\n",
      "|     8|       30047|    2003|04/19/2004 12:00:...|   Andover|  10 CHESTER BRKS LN|        56600|     80000|            70.75| Vacant Land|             NA|               0|                  NA|\n",
      "|     9|       40003|    2004|10/18/2004 12:00:...|   Andover|  10 CHESTER BRKS LN|       194100|    446639| 43.4579156768666| Residential|  Single Family|               7|                  NA|\n",
      "|    10|       70005|    2007|11/19/2007 12:00:...|   Andover|10 CHESTER BROOKS LN|       313400|    425000|0.737411764705882| Residential|  Single Family|              NA|                  NA|\n",
      "|    11|       40042|    2004|05/23/2005 12:00:...|   Andover|       10 DOGWOOD DR|       218700|    440000| 49.7045454545455| Residential|  Single Family|               7|                  NA|\n",
      "|    12|       13021|    2013|06/16/2014 12:00:...|   Andover|  10 HICKORY HILL DR|       109400|    165000|            0.663| Residential|  Single Family|              NA|                  NA|\n",
      "|    13|       40035|    2004|04/01/2005 12:00:...|   Andover|    10 HICKORY HL DR|        77900|    184000| 42.3369565217391| Residential|  Single Family|               0|                  NA|\n",
      "|    14|       14044|    2014|09/28/2015 12:00:...|   Andover|   10 PINE RIDGE  DR|       108700|    128368|      0.846784245| Residential|  Single Family|              NA|                  NA|\n",
      "|    15|       50027|    2005|03/16/2006 12:00:...|   Andover|     10 SHODDY ML RD|        84900|    215000| 39.4883720930233| Residential|  Single Family|               1|                  NA|\n",
      "|    16|       30063|    2003|07/15/2004 12:00:...|   Andover|       101 GILEAD RD|       104000|    218000|             47.7| Residential|  Single Family|               0|                  NA|\n",
      "|    17|       14035|    2014|08/10/2015 12:00:...|   Andover|        101 WALES RD|       164000|    230000|      0.713043478| Residential|  Single Family|              NA|                  NA|\n",
      "|    18|      900008|    2009|12/14/2009 12:00:...|   Andover|      101 WALES ROAD|       186400|    248500| 0.75010060362173| Residential|  Single Family|              NA|                  NA|\n",
      "|    19|       40650|    2004|08/18/2005 12:00:...|   Andover|       102 GILEAD RD|       111100|    279900| 39.6927474097892| Residential|  Single Family|               0|                  NA|\n",
      "|    20|       30055|    2003|01/06/2004 12:00:...|   Andover|         104 WEST ST|        52300|    314867|            16.61| Residential|  Single Family|               7|                  NA|\n",
      "+------+------------+--------+--------------------+----------+--------------------+-------------+----------+-----------------+------------+---------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.real_estate_sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+--------------------+----------+--------------------+-------------+----------+-----------------+------------+---------------+----------------+--------------------+\n",
      "|    ID|SerialNumber|ListYear|        DateRecorded|      Town|             Address|AssessedValue|SaleAmount|       SalesRatio|PropertyType|ResidentialType|      NonUseCode|             Remarks|\n",
      "+------+------------+--------+--------------------+----------+--------------------+-------------+----------+-----------------+------------+---------------+----------------+--------------------+\n",
      "|815906|      170177|    2017|04/05/1999 12:00:...|New London|      293 PEQUOT AVE|       132440|    252500|            0.525|        null|     Two Family|            null|                null|\n",
      "|     2|      900035|    2009|07/20/2010 12:00:...|   Andover|     1 DOGWOOD DRIVE|        55600|     99000|0.561616161616162| Vacant Land|             NA|              NA|                  NA|\n",
      "|     3|       14011|    2014|01/14/2015 12:00:...|   Andover|     1 JUROVATY LANE|       153100|    190000|      0.805789474| Residential|  Single Family|              NA|                  NA|\n",
      "|     4|       80009|    2008|01/21/2009 12:00:...|   Andover|         1 ROSE LANE|       116600|    138900|0.839452843772498| Residential|  Single Family|              NA|                  NA|\n",
      "|     5|       15006|    2015|11/30/2015 12:00:...|   Andover|         1 ROSE LANE|       102900|     50000|            2.058| Residential|  Single Family|14 - Foreclosure|PROPERTY WAS OWNE...|\n",
      "|     6|       20030|    2002|04/24/2003 12:00:...|   Andover|       10 BAUSOLA RD|        91800|    189900| 48.3412322274881| Residential|  Single Family|               0|                  NA|\n",
      "|815907|      173165|    2017|08/01/2001 12:00:...|   Shelton|        28 SUNSET DR|       126980|    238000|            0.534|        null|  Single Family|            null|                null|\n",
      "|     8|       30047|    2003|04/19/2004 12:00:...|   Andover|  10 CHESTER BRKS LN|        56600|     80000|            70.75| Vacant Land|             NA|               0|                  NA|\n",
      "|     9|       40003|    2004|10/18/2004 12:00:...|   Andover|  10 CHESTER BRKS LN|       194100|    446639| 43.4579156768666| Residential|  Single Family|               7|                  NA|\n",
      "|    10|       70005|    2007|11/19/2007 12:00:...|   Andover|10 CHESTER BROOKS LN|       313400|    425000|0.737411764705882| Residential|  Single Family|              NA|                  NA|\n",
      "|    11|       40042|    2004|05/23/2005 12:00:...|   Andover|       10 DOGWOOD DR|       218700|    440000| 49.7045454545455| Residential|  Single Family|               7|                  NA|\n",
      "|    12|       13021|    2013|06/16/2014 12:00:...|   Andover|  10 HICKORY HILL DR|       109400|    165000|            0.663| Residential|  Single Family|              NA|                  NA|\n",
      "|    13|       40035|    2004|04/01/2005 12:00:...|   Andover|    10 HICKORY HL DR|        77900|    184000| 42.3369565217391| Residential|  Single Family|               0|                  NA|\n",
      "|    14|       14044|    2014|09/28/2015 12:00:...|   Andover|   10 PINE RIDGE  DR|       108700|    128368|      0.846784245| Residential|  Single Family|              NA|                  NA|\n",
      "|    15|       50027|    2005|03/16/2006 12:00:...|   Andover|     10 SHODDY ML RD|        84900|    215000| 39.4883720930233| Residential|  Single Family|               1|                  NA|\n",
      "|    16|       30063|    2003|07/15/2004 12:00:...|   Andover|       101 GILEAD RD|       104000|    218000|             47.7| Residential|  Single Family|               0|                  NA|\n",
      "|    17|       14035|    2014|08/10/2015 12:00:...|   Andover|        101 WALES RD|       164000|    230000|      0.713043478| Residential|  Single Family|              NA|                  NA|\n",
      "|    18|      900008|    2009|12/14/2009 12:00:...|   Andover|      101 WALES ROAD|       186400|    248500| 0.75010060362173| Residential|  Single Family|              NA|                  NA|\n",
      "|    19|       40650|    2004|08/18/2005 12:00:...|   Andover|       102 GILEAD RD|       111100|    279900| 39.6927474097892| Residential|  Single Family|               0|                  NA|\n",
      "|    20|       30055|    2003|01/06/2004 12:00:...|   Andover|         104 WEST ST|        52300|    314867|            16.61| Residential|  Single Family|               7|                  NA|\n",
      "+------+------------+--------+--------------------+----------+--------------------+-------------+----------+-----------------+------------+---------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.real_estate_sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating New Session (Indepth Internal and optional topic)\n",
    "- If SparkSession.builder is used, the below way does NOT return new app. It returns the same app which was previously created. It is by design and single responsibility principle\n",
    "- As discussed before there can only be one SparkContext per JVM and SparkSession is a wrapper around SparkContext\n",
    "- A new session is created by calling .newSession() on existing session only\n",
    "- Feel free to open the spark UI and you can see that \"MyApp\" will be the name of the app\n",
    "- The way to have MyApp1 would be to execute spark.stop() and start new\n",
    "\n",
    "**Although configuration option spark.driver.allowMultipleContexts exists, it is misleading because usage of multiple Spark contexts is discouraged. This option is used only for Spark internal tests and is not supposed to be used in user programs. You can get unexpected results while running more than one Spark context in a single JVM.**  \n",
    "[StackOverFlow](https://stackoverflow.com/questions/32827333/spark-multiple-contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11ac778d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_new = SparkSession.builder.config(conf=config).master(\"local\").appName(\"MyApp1\").getOrCreate()\n",
    "spark_new # This is the same session as spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark == spark_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One more slightly complex example\n",
    "- Top 3 PropertyType by TotalSales\n",
    "- Observe that physical plan for both is exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- SerialNumber: string (nullable = true)\n",
      " |-- ListYear: string (nullable = true)\n",
      " |-- DateRecorded: string (nullable = true)\n",
      " |-- Town: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- AssessedValue: string (nullable = true)\n",
      " |-- SaleAmount: string (nullable = true)\n",
      " |-- SalesRatio: string (nullable = true)\n",
      " |-- PropertyType: string (nullable = true)\n",
      " |-- ResidentialType: string (nullable = true)\n",
      " |-- NonUseCode: string (nullable = true)\n",
      " |-- Remarks: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_real_estate = spark.read.load(\"../Real_Estate_Sales_2001-2017.csv\", \n",
    "                                 format=\"csv\", sep=\",\", header=\"true\")\n",
    "df_real_estate.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \\ to break multiple lines\n",
    "from pyspark.sql.types import * # for DoubleType()\n",
    "df_top_3 = df_real_estate.withColumn(\"SaleAmount\",df_real_estate.SaleAmount.cast(DoubleType())) \\\n",
    ".groupBy(\"PropertyType\").sum(\"SaleAmount\") \\\n",
    ".withColumnRenamed(\"sum(SaleAmount)\",\"TotalSales\") \\\n",
    ".sort(\"TotalSales\",ascending=False).limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|PropertyType|     TotalSales|\n",
      "+------------+---------------+\n",
      "| Residential|204614966269.29|\n",
      "|  Commercial| 30602284248.00|\n",
      "|       Condo| 25301417810.00|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_top_3.withColumn('TotalSales', df_top_3.TotalSales.cast(DecimalType(18, 2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_estate.createOrReplaceTempView(\"real_estate_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql_top_3 = spark.sql(\"select PropertyType, sum(SaleAmount) as TotalSales from real_estate_sales GROUP BY PropertyType ORDER BY sum(SaleAmount) DESC LIMIT 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|PropertyType|     TotalSales|\n",
      "+------------+---------------+\n",
      "| Residential|204614966269.29|\n",
      "|  Commercial| 30602284248.00|\n",
      "|       Condo| 25301417810.00|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql_top_3.withColumn('TotalSales', df_sql_top_3.TotalSales.cast(DecimalType(18, 2))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=3, orderBy=[TotalSales#911 DESC NULLS LAST], output=[PropertyType#180,TotalSales#911])\n",
      "+- *(2) HashAggregate(keys=[PropertyType#180], functions=[sum(SaleAmount#880)])\n",
      "   +- Exchange hashpartitioning(PropertyType#180, 200)\n",
      "      +- *(1) HashAggregate(keys=[PropertyType#180], functions=[partial_sum(SaleAmount#880)])\n",
      "         +- *(1) Project [cast(SaleAmount#178 as double) AS SaleAmount#880, PropertyType#180]\n",
      "            +- *(1) FileScan csv [SaleAmount#178,PropertyType#180] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/pmacharl/git-projects/personal/github.com/data_analysis_pandas_spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<SaleAmount:string,PropertyType:string>\n"
     ]
    }
   ],
   "source": [
    "df_top_3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=3, orderBy=[aggOrder#930 DESC NULLS LAST], output=[PropertyType#180,TotalSales#928])\n",
      "+- *(2) HashAggregate(keys=[PropertyType#180], functions=[sum(cast(SaleAmount#178 as double))])\n",
      "   +- Exchange hashpartitioning(PropertyType#180, 200)\n",
      "      +- *(1) HashAggregate(keys=[PropertyType#180], functions=[partial_sum(cast(SaleAmount#178 as double))])\n",
      "         +- *(1) FileScan csv [SaleAmount#178,PropertyType#180] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/pmacharl/git-projects/personal/github.com/data_analysis_pandas_spar..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<SaleAmount:string,PropertyType:string>\n"
     ]
    }
   ],
   "source": [
    "df_sql_top_3.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
