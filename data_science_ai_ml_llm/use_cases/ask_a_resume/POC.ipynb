{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce7209d-f1a0-4a82-9a2a-87ff8d3d7234",
   "metadata": {},
   "source": [
    "# Use Case Description\n",
    "Recruiters in HRTech industry / head hunters deal with enormouse amounts of data in job descriptions and resumes. Matching a job description with resume has many stages and predominantly recruiters gather keywords (their own understanding which can be limited) from job description, and hunt for those in job resumes to get to the next step of reaching out to candidates. Few noteworthy points\n",
    "- The speed at which a recruiter works is a function of their domain knowledge, human  extraction of keywords / semantic forms (e.g. \"java\" is semantic to spring framework)\n",
    "- There is generally 1000 to 10k resumes for some jobs and humanly applying the first filter (aka. first pass filter to potentially narrows down resumes) might take days to weeks even for the most experienced recruiter\n",
    "- Any human can make errors when matching and their own biases come into play when applying the first filter (errors can be not matching \"spring boot\" to \"java\" OR filtering out the resume because they don't find \"java\" mentioned enough number of times etc.)\n",
    "- Result can be loss of excellent talent, time delay = $ cost and many more  \n",
    "\n",
    "<strong>For the first pass filter i.e </strong> <i>looking for a keyword in a resume in the form of question-answering</i> and augment the human search with LLM + AI semantic search</strong> seems a great start\n",
    "\n",
    "- We will use my own resume from [linkedin](https://www.linkedin.com/in/pradeepmacharla/)\n",
    "- Note: The pdf version is not available in this repo, instead use your own resume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250f1ed-e9e2-4ae3-885d-ba4d03cfc9ed",
   "metadata": {},
   "source": [
    "# Technical\n",
    "- This notebook implements the same workflow model that [privateGPT](https://github.com/imartinez/privateGPT) has done and packaged into a nice python script (Path1: ingest documents > Tokenize > Embeddings/Vectors > VectorStore , Path2: Input Query > Embed and find similarity score using vecstore store > Pass this as context to LLM > Respond)\n",
    "- privateGPT uses duckdb, we are using FAISS and pickling (very much like sqlite)\n",
    "- <strong>Ubuntu 20.04 OS with no GPU is used for this</strong> on 8gb ram, 4 cpus\n",
    "- Python 3.10.12 has been used as you can see below (It doesn't matter whether you manage runtime with pyenv or conda - both should produce identical results)\n",
    "- The below libraries and frameworks are used in this notebook and app.py  \n",
    "- Most of the code is adopted from [blog](https://blog.streamlit.io/langchain-tutorial-4-build-an-ask-the-doc-app/)\n",
    "- We did not use OPENAI though because most of them might not want to end up spending $ when doing POC for self or learning. \n",
    "- Huggingface account is needed as we download a model, however once downloaded we no longer need it\n",
    "- There are many parameters and hyper parameter values that we chose (e.g. max_tokens) - Knowing which value to set is highly dependent on domain and use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33c4f79-9fe1-44d2-a21d-8d1249b9eace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6848ee3-b2cb-4da7-9f61-f1d26be091d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install streamlit==1.24.1\n",
    "!pip3 install PyPDF2==3.0.1\n",
    "!pip3 install langchain==0.0.234\n",
    "!pip3 install faiss-cpu==1.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eeddb2-9913-4a7c-8b36-4d7a966defc3",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0524354-5fcd-43be-912f-fb6ba26da74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "import pickle\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056a294-64dc-41c5-a6ed-a34a8657e7aa",
   "metadata": {},
   "source": [
    "### HuggingFace initial\n",
    "- For using transformers library to download models or interact with huggingface hub...\n",
    "- On a shell prompt `huggingface-cli login` - complete the process if you wish to run outside this notebook\n",
    "- Access token can be got using [help](https://huggingface.co/docs/hub/security-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb31cd-592a-4072-95df-fe23f67cd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook_login() doesn't work use interpretor_login\n",
    "from huggingface_hub import notebook_login, interpreter_login\n",
    "notebook_login()\n",
    "# interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052e411b-d15a-4701-940a-41d45eea8f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Substitute with whatever pdf you want to ask questions on \n",
    "SAMPLE_RESUME = \"pradeep_resume.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42dbeeb4-b765-41f5-a3c0-fa9703c39917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3540"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note we are not doing any preprocessing, cleaning etc, as text from PDF can contain special characters aka. noise for data science models\n",
    "# However we are using pretrained LLMs directly (tokenizers and embeddings hopefully take care - lets see below) \n",
    "pdf_reader = PdfReader(SAMPLE_RESUME)\n",
    "text = \"\"\n",
    "for page in pdf_reader.pages:\n",
    "    text += page.extract_text()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd91f98a-db52-4f65-b2dd-f42cde10ffb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len)\n",
    "\n",
    "chunks = text_splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630c8b7-d762-475b-a7c9-3c691b449e1c",
   "metadata": {},
   "source": [
    "## Create vector store and index\n",
    "- if exists, reuse, else create\n",
    "- This might take a min (depending on content) to run\n",
    "- We will use HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9159bf0-f945-4809-940b-d04c7bc27418",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store_name = \"myvector_store\"\n",
    "if os.path.exists(f\"{store_name}.pkl\"):\n",
    "        with open(f\"{store_name}.pkl\", \"rb\") as f:\n",
    "            vector_store = pickle.load(f)\n",
    "else:\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    vector_store = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "    with open(f\"{store_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vector_store, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6e42e-897f-4903-bbe7-07ec2e20c571",
   "metadata": {},
   "source": [
    "### Model\n",
    "- As you can see we are not using openai, but using a locally downloaded GPT4All\n",
    "- There are many models you can choose from and they have trade-offs mostly speed, accuracy, performance dimensions\n",
    "- Go to [GPT4All](https://gpt4all.io/index.html) and use Model explorer to download the model\n",
    "- We used orca-mini-3b.ggmlv3.q4_0.bin which is about 1.8gb, which most personal laptops should be able to handle\n",
    "- Response should come back in under 10-20 seconds (if you guessed it will be nano or pico second using GPU - of course smarty pants!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40118864-b323-4173-8db4-7d1c8f772cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/ubuntu/Downloads/orca-mini-3b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/ubuntu/Downloads/orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.06 MB\n",
      "llama_model_load_internal: mem required  = 2862.72 MB (+  682.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The contact information in the input document is \"macharla.pradeep.kumar@gmai l.com\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the contact address in the input document?\"\n",
    "docs = vector_store.similarity_search(query=query, k=2)\n",
    "\n",
    "llm = GPT4All(model=\"/home/ubuntu/Downloads/orca-mini-3b.ggmlv3.q4_0.bin\",max_tokens=2048)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "response = chain.run(input_documents=docs,question=query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52169b5-f83e-43fe-b949-d9ad943cd256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/ubuntu/Downloads/orca-mini-3b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/ubuntu/Downloads/orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.06 MB\n",
      "llama_model_load_internal: mem required  = 2862.72 MB (+  682.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sure, here are some of the skills listed by Pradeep Macharla on his LinkedIn profile: IT Leadership | Strategic Planning | P&L | Team Development | Vendor Negotiation & Management \\nData Solutions Design & Development | Hands-on Data Analysis, Engineering and Science DOMAIN SKILLS Technical - Business Intelligence | Enterprise Software Development| Big Data & AI/ML | Software Automation Solution Design, implementation and maintenance | Digital Marketing Tools | ETL Tools INDIVIDUAL ACHIEVEMENTS \\n1) Book Author - Android Continuous Integration - https://www.amazon.com/Android-Continuous-Integration-Automating-Software-Development/dp/032159487X \\nDaily Mail and General Trust plc Quality Assurance Lead May 2010 - August 2012 (2 years 4 months) Naviance, a subsidiary of hobsons, in turn the subsidiary of Daily Mail and Trust PLC, is the premier education solution provider for k-12 schools in the US. www.naviance.com www.hobsons.com www.dmgt.co.uk Page 3 of'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Can you list all skills please?\"\n",
    "docs = vector_store.similarity_search(query=query, k=2)\n",
    "\n",
    "llm = GPT4All(model=\"/home/ubuntu/Downloads/orca-mini-3b.ggmlv3.q4_0.bin\",max_tokens=2048)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "response = chain.run(input_documents=docs,question=query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2a669-e0f7-4172-a85e-a5945e466b18",
   "metadata": {},
   "source": [
    "## Change Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "044d7172-65e6-426c-9d93-686e8ad452b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /home/ubuntu/Downloads/ggml-gpt4all-j-v1.3-groovy.bin\n",
      "gptj_model_load: loading model from '/home/ubuntu/Downloads/ggml-gpt4all-j-v1.3-groovy.bin' - please wait ...\n",
      "gptj_model_load: n_vocab = 50400\n",
      "gptj_model_load: n_ctx   = 2048\n",
      "gptj_model_load: n_embd  = 4096\n",
      "gptj_model_load: n_head  = 16\n",
      "gptj_model_load: n_layer = 28\n",
      "gptj_model_load: n_rot   = 64\n",
      "gptj_model_load: f16     = 2\n",
      "gptj_model_load: ggml ctx size = 5401.45 MB\n",
      "gptj_model_load: kv self size  =  896.00 MB\n",
      "gptj_model_load: ................................... done\n",
      "gptj_model_load: model size =  3609.38 MB / num tensors = 285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Pradeep has the following skills listed on his LinkedIn profile: Project Delivery Service Delivery Account Management Certifications AWS Certified Solutions Architect - AssociatePradeep Macha'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Can you list all skills for pradeep?\"\n",
    "docs = vector_store.similarity_search(query=query, k=2)\n",
    "\n",
    "llm = GPT4All(model=\"/home/ubuntu/Downloads/ggml-gpt4all-j-v1.3-groovy.bin\",max_tokens=2048)\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "response = chain.run(input_documents=docs,question=query)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
