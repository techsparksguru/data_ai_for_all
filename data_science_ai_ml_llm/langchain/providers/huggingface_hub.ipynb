{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af249640-3d07-413e-b010-6ed89a4610e3",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- https://python.langchain.com/docs/integrations/providers/huggingface\n",
    "- There exists two Hugging Face LLM wrappers, one for a local pipeline and one for a model hosted on Hugging Face Hub. Note that these wrappers only work for models that support the following tasks: text2text-generation, text-generation\n",
    "- Local pipeline wrapper & embeddings `from langchain.llms import HuggingFacePipeline` & `from langchain.embeddings import HuggingFaceEmbeddings`\n",
    "- Wrapper for a model hosted on hub & embeddings`from langchain.llms import HuggingFaceHub` & `from langchain.embeddings import HuggingFaceHubEmbeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41087b17-6cc8-4237-ae28-96c0c38ee10f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6812e2-68da-4756-b521-816de720377d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install langchain==0.0.242\n",
    "!pip3 install chromadb==0.4.2\n",
    "!pip3 install huggingface_hub==0.16.4\n",
    "!pip3 install transformers==4.28.1\n",
    "!pip3 install sentence-transformers==2.2.2\n",
    "!pip3 install datasets==2.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe6ea78c-ce7c-4167-940a-9232b48a0f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f04ce11a-67ef-4abb-ad5c-0676fd2af743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Token:  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541c6a1-4d93-4ad5-a838-14d7a3ebb5ec",
   "metadata": {},
   "source": [
    "# Hugging Face Hub examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5972a9a9-7fcf-4dc3-a516-522fa1df1e14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994 was the 20th FIFA World Cup. France won the World Cup in 1994. The answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "repo_id = \"google/flan-t5-xxl\" \n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "218af20c-4eed-4334-b473-927493cc5e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First of all, the world cup was won by the Germany. Then the Argentina won the world cup in 2022. So, the Argentina won the world cup in 1994.\n",
      "\n",
      "\n",
      "Question: Who\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"databricks/dolly-v2-3b\"\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6d720-bf68-4c83-afb4-068b685f2f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_id = \"Writer/camel-5b-hf\"  # See https://huggingface.co/Writer for other options\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119ea68-5ec6-457e-a284-aec6a965ed8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_id = \"Salesforce/xgen-7b-8k-base\"\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15e720-2643-4402-b4ee-b12b68b0c09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo_id = \"tiiuae/falcon-40b\"\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653eb572-bda9-4d37-82ec-21233284ff52",
   "metadata": {},
   "source": [
    "## Local Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e54a43-5dbc-4eeb-9f98-f5ec698f5e90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Electroencephalography (EEG) is a type of brain imaging that uses a computerized computer to measure electrical activity in the brain\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 64},\n",
    ")\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729ecc3-41f6-499c-9439-af6c561f7ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
